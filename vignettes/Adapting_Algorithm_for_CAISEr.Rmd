---
title: "Adapting Algorithms for CAISEr"
author: "Felipe Campelo"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

# Introduction
This is a short guide to adapting existing algorithms and problem instances for 
running an experiment using CAISEr. In this document, we cover:

- Definition of instance lists
- Adaptation of existing algorithms
- Some stuff you can do with the results

A general description of the CAISE methodology is available in our paper^[F. 
Campelo, F. Takahashi, "Sample size estimation for power and accuracy in the 
experimental comparison of  algorithms", under review.]

## Assembling an instance list
As stated in the documentation of both `run_experiment` and `calc_nreps2`, each 
instance must be "_a named list containing all relevant parameters that 
define the problem instance. This list must contain at least the field
`instance$FUN`, with the name of the problem instance function, that is, a
routine that calculates y = f(x). If the instance requires additional
parameters, these must also be provided as named fields_". 

In this document we assume that our problem class of interest is 
well-represented by problems UF1 - UF7 available in package 
[smoof](https://CRAN.R-project.org/package=smoof), for dimensions between 10 
and 40. For these instances to work with the `MOEADr::moead()` routine some 
manipulation is necessary, but the instance list in this case is simply a list 
with each element containing the name of the routine as field `$FUN` (since 
all function names are different, no need for aliases).

```{r, cache=TRUE}
suppressPackageStartupMessages(library(smoof))
suppressPackageStartupMessages(library(MOEADr))

### Build training instances and assemble instances list
fname   <- paste0("UF_", 1:7)
dims    <- c(10:40)
allfuns <- expand.grid(fname, dims, stringsAsFactors = FALSE)

# Assemble instances list
Instance.list <- vector(nrow(allfuns), mode = "list")
for (i in 1:length(Instance.list)){
  Instance.list[[i]]$FUN <- paste0(allfuns[i,1], "_", allfuns[i,2])
}

# Build the instances in instances.list 
# (so that they can be properly used)
for (i in 1:nrow(allfuns)){
  assign(x = Instance.list[[i]]$FUN,
     value = MOEADr::make_vectorized_smoof(prob.name  = "UF",
                    dimensions = allfuns[i, 2],
                    id = as.numeric(strsplit(allfuns[i, 1], "_")[[1]][2])))
}
```

## Adaptation of existing algorithms
We will use the MOEA/D implementation available in the [MOEADr](https://CRAN.R-project.org/package=MOEADr) package as our base 
algorithm, and assume that we are interested in comparing the performance of 
two versions of this algorithm: the original MOEA/D and the MOEA/D-DE (see the 
documentation of [MOEADr](https://CRAN.R-project.org/package=MOEADr) and 
references therein for details of these methods.) as solvers of a particular 
problem class (to be defined later in this document). The performance of each 
algorithm on each instance will be measured according to an indicator known as 
inverted generational distance (details 
[here](http://ieeexplore.ieee.org/document/1197687/)). With this indicator, 
_smaller = better_.

```{r, cache=TRUE}
# Prepare function for algorithm 1:
algorithm <- function(type, instance){
  # Input parameters:
  #     - type (variant to use: "original" or "moead.de")
  #     - instance (instance to be solved, e.g., Instance.list[[1]])
  # All other parameters are set internally

  ## Adapt the instance to the MOEADr problem format
  fdef    <- unlist(strsplit(instance$FUN, split = "_"))
  uffun   <- smoof::makeUFFunction(dimensions = as.numeric(fdef[3]),
                                   id         = as.numeric(fdef[2]))
  fattr   <- attr(uffun, "par.set")
  problem <- list(name       = instance$FUN,
                  xmin       = fattr$pars$x$lower,
                  xmax       = fattr$pars$x$upper,
                  m          = attr(uffun, "n.objectives"))


  # Run algorithm on "instance"
  algo.preset <- MOEADr::preset_moead(type)
  algo.preset$decomp$H <- 99
  algo.preset$stopcrit[[1]]$name <- "maxeval"
  algo.preset$stopcrit[[1]]$maxeval <- 2000 * fattr$pars$x$len
  out <- MOEADr::moead(preset = algo.preset, problem = problem,
                       showpars = list(show.iters = "none"))

  # Read reference data and calculate IGD value
  Yref  <- as.matrix(read.table(paste0("../inst/extdata/pf_data/",
                                       fdef[1], fdef[2], ".dat")))

  # Return quality value as field "value" in the output list
  return(list(value = MOEADr::calcIGD(Y = out$Y, Yref = Yref)))
}

# Assemble Algorithm.list
Algorithm.list <- list(list(FUN   = "algorithm", 
                            alias = "Algorithm 1", 
                            type  = "original"), 
                       list(FUN   = "algorithm", 
                            alias = "Algorithm 1", 
                            type  = "moead.de"))
```

## Running an experiment using CAISEr

With the definition above it is possible now to run a simple experiment 
using the iterative sample size determination implemented in CAISEr. For 
that, all we have to do is define the desired experimental parameters and 
use `run_experiment()`:

```{r, cache=TRUE}
#library(CAISEr)
my.results <- run_experiment(Instance.list = Instance.list,
                             Algorithm.list = Algorithm.list,
                             power = 0.8,      # Desired power: 80%
                             d = 0.5,          # to detect differences greater
                                               # than 0.5 standard deviations
                             sig.level = 0.05, # at a 95% confidence level. 
                             se.max = 0.05,    # Measurement error: 5% 
                             dif = "perc",     # on the paired percent 
                                               # differences of means,
                             method = "boot",  # calculated using bootstrap
                             nstart = 20,      # Start with 20 runs/algo/inst
                             nmax   = 200,     # and do no more than 200/inst
                             seed   = 1234)    # PRNG seed (for reproducibility)
```

After that we can interrogate the results and perform inference, if we are so 
inclined. 
For instance, we can check if our sample of paired differences in performance 
is (at least approximately) Normal, so that we can assume a Normal sampling 
distribution of the means and use a t test with a clean conscience:
inference on the mean of the paired diffences in performance 
can be easily done with a t test.

```{r, cache=TRUE}
# Take a look at the data summary:
my.results$data.summary

suppressPackageStartupMessages(library(car))
qqPlot(my.results$data.summary$phi.j, pch = 20, las = 1)
```

It is interesting to notice that in most tested instances we see negative 
values of `phi_j`, which suggests an advantage of the MOEA/D-De over the 
original MOEA/D. MOEA/D-DE also seems to require smaller sample sizes in most 
instances, suggesting a smaller variance of performance. The first observation 
can be tested with a t test:

```{r, cache=TRUE}
t.test(my.results$data.summary$phi.j)
```

